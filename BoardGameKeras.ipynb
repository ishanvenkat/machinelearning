{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bnSXyCNN5C0E",
        "outputId": "369654b3-dc15-42f2-fc37-75e4fd1757e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.91.82)] [Connecting to security.ubuntu.com (185.125.190\u001b[0m\r                                                                                                    \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Ign:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Hit:7 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "43 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "xorg-dev is already the newest version (1:7.7+23ubuntu2).\n",
            "swig is already the newest version (4.0.2-1ubuntu1).\n",
            "libpq-dev is already the newest version (14.12-0ubuntu0.22.04.1).\n",
            "libsdl2-dev is already the newest version (2.0.20+dfsg-2ubuntu1.22.04.1).\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.11).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 43 not upgraded.\n",
            "Requirement already satisfied: tf-agents in /usr/local/lib/python3.10/dist-packages (0.19.0)\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.10/dist-packages (3.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (1.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (2.2.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (0.5.0)\n",
            "Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (1.25.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf-agents) (9.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (3.20.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions==4.5.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (4.5.0)\n",
            "Requirement already satisfied: pygame==2.1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (2.1.3)\n",
            "Requirement already satisfied: tensorflow-probability~=0.23.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (0.23.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (0.0.8)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.23.0->tf-agents) (4.4.2)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.23.0->tf-agents) (0.6.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.23.0->tf-agents) (0.1.8)\n",
            "Collecting gym~=0.21.0\n",
            "  Using cached gym-0.21.0.tar.gz (1.5 MB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Requirement already satisfied: gym[accept-rom-license,atari,box2d] in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Collecting gym[accept-rom-license,atari,box2d]\n",
            "  Using cached gym-0.26.2.tar.gz (721 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari,box2d]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari,box2d]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[accept-rom-license,atari,box2d]) (0.0.8)\n",
            "Collecting ale-py~=0.8.0 (from gym[accept-rom-license,atari,box2d])\n",
            "  Using cached ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "Collecting box2d-py==2.3.5 (from gym[accept-rom-license,atari,box2d])\n",
            "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygame==2.1.0 (from gym[accept-rom-license,atari,box2d])\n",
            "  Using cached pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Collecting swig==4.* (from gym[accept-rom-license,atari,box2d])\n",
            "  Using cached swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2 (from gym[accept-rom-license,atari,box2d])\n",
            "  Using cached AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.0->gym[accept-rom-license,atari,box2d]) (6.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.0->gym[accept-rom-license,atari,box2d]) (4.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,box2d]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,box2d]) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,box2d]) (4.66.4)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,box2d])\n",
            "  Using cached AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,box2d]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,box2d]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,box2d]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari,box2d]) (2024.7.4)\n",
            "Building wheels for collected packages: box2d-py, gym, AutoROM.accept-rom-license\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2349115 sha256=19d9717f15d9a2402f65ead7676056d982a2af105d4a45cd3c1ab270441a09c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827620 sha256=211ac5638f55fd9484760587d3a6936352dc6faeb201ac83f27a753fe9411dc9\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/22/6d/3e7b32d98451b4cd9d12417052affbeeeea012955d437da1da\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446664 sha256=1769d597712fe14e4a3b1d4569e31ae32a4f992dd71ec7792a95e84ddf4a2432\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built box2d-py gym AutoROM.accept-rom-license\n",
            "Installing collected packages: swig, box2d-py, pygame, gym, ale-py, AutoROM.accept-rom-license, autorom\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.1.3\n",
            "    Uninstalling pygame-2.1.3:\n",
            "      Successfully uninstalled pygame-2.1.3\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.23.0\n",
            "    Uninstalling gym-0.23.0:\n",
            "      Successfully uninstalled gym-0.23.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.0.9 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\n",
            "tf-agents 0.19.0 requires gym<=0.23.0,>=0.17.0, but you have gym 0.26.2 which is incompatible.\n",
            "tf-agents 0.19.0 requires pygame==2.1.3, but you have pygame 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.4.2 box2d-py-2.3.5 gym-0.26.2 pygame-2.1.0 swig-4.2.1\n",
            "No GPU was detected. CNNs can be very slow without a GPU.\n",
            "Go to Runtime > Change runtime and select a GPU hardware accelerator.\n"
          ]
        }
      ],
      "source": [
        "# @title Setup\n",
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Is this notebook running on Colab or Kaggle?\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
        "\n",
        "if IS_COLAB or IS_KAGGLE:\n",
        "    !apt update && apt install -y libpq-dev libsdl2-dev swig xorg-dev xvfb\n",
        "    %pip install -U tf-agents pyvirtualdisplay\n",
        "    %pip install -U gym~=0.21.0\n",
        "    %pip install -U gym[box2d,atari,accept-rom-license]\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
        "    if IS_KAGGLE:\n",
        "        print(\"Go to Settings > Accelerator and select GPU.\")\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# To get smooth animations\n",
        "import matplotlib.animation as animation\n",
        "mpl.rc('animation', html='jshtml')\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"rl\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iI9vIvnX5c1y"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import keras\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "from gym import Env\n",
        "from gym import spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtpdFJy_v2yP"
      },
      "outputs": [],
      "source": [
        "# game board values\n",
        "NOTHING = 0\n",
        "PLAYER = 1\n",
        "WIN = 2\n",
        "LOSE = 3\n",
        "\n",
        "# action values\n",
        "UP = 0\n",
        "DOWN = 1\n",
        "LEFT = 2\n",
        "RIGHT = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoG-LA0iv9La"
      },
      "outputs": [],
      "source": [
        "class GameBoardEnv(Env):\n",
        "    def __init__(self):\n",
        "      # custom class variable used to display the reward earned\n",
        "      self.cumulative_reward = 0\n",
        "      #\n",
        "      # set the initial state to a flattened 6x6 grid with a randomly\n",
        "      # placed entry, win, and player\n",
        "      #\n",
        "      self.state = [NOTHING] * 36\n",
        "      # self.player_position = random.randrange(0, 36)\n",
        "      # self.win_position = random.randrange(0, 36)\n",
        "      # self.lose_position = random.randrange(0, 36)\n",
        "      self.player_position = 5\n",
        "      self.win_position = 15\n",
        "      self.lose_position = 30\n",
        "\n",
        "      # make sure the player, win, and lose points aren't\n",
        "      # overlapping each other\n",
        "      while self.win_position == self.player_position:\n",
        "          self.win_position = random.randrange(0, 36)\n",
        "      while self.lose_position == self.win_position or self.lose_position == self.player_position:\n",
        "          self.lose_position = random.randrange(0, 36)\n",
        "\n",
        "      self.state[self.player_position] = PLAYER\n",
        "      self.state[self.win_position] = LOSE\n",
        "      self.state[self.lose_position] = WIN\n",
        "\n",
        "      # convert the python array into a numpy array\n",
        "      # (This is needed since Gym expects the state to be this way)\n",
        "      self.state = np.array(self.state, dtype=np.int16)\n",
        "      # observation space (valid ranges for observations in the state)\n",
        "      self.observation_space = spaces.Box(0, 3, [36,], dtype=np.int16)\n",
        "\n",
        "      # valid actions:\n",
        "      #   0 = up\n",
        "      #   1 = down\n",
        "      #   2 = left\n",
        "      #   3 = right\n",
        "      # spaces.Discrete(4) is a shortcut for defining the actions 0-3\n",
        "      self.action_space = spaces.Discrete(4)\n",
        "\n",
        "    def step(self, action):\n",
        "      # placeholder for debugging information\n",
        "      info = {}\n",
        "      # set default values for done, reward, and the player position\n",
        "      #before taking the action\n",
        "      done = False\n",
        "      previous_position = self.player_position\n",
        "      #\n",
        "      # take the action by moving the player\n",
        "      #\n",
        "      # this section can be a bit confusing, but\n",
        "      # just trust that they move the agent and prevent\n",
        "      # it from moving off of the grid\n",
        "      #\n",
        "      if action == UP:\n",
        "          if (self.player_position - 6) >= 0:\n",
        "              self.player_position -= 6\n",
        "      elif action == DOWN:\n",
        "          if (self.player_position + 6) < 36:\n",
        "              self.player_position += 6\n",
        "      elif action == LEFT:\n",
        "          if (self.player_position % 6) != 0:\n",
        "              self.player_position -= 1\n",
        "      elif action == RIGHT:\n",
        "          if (self.player_position % 6) != 5:\n",
        "              self.player_position += 1\n",
        "      else:\n",
        "          # check for invalid actions\n",
        "          raise Exception(\"invalid action\")\n",
        "      #\n",
        "      # check for win/lose conditions and set reward\n",
        "      #\n",
        "      if self.state[self.player_position] == WIN:\n",
        "          reward = 1.0\n",
        "          done = True\n",
        "\n",
        "          # this section is for display purposes\n",
        "          # clear_screen()\n",
        "          self.cumulative_reward += reward\n",
        "          print(f'Cumulative Reward: {self.cumulative_reward}, Result: WIN')\n",
        "\n",
        "      elif self.state[self.player_position] == LOSE:\n",
        "          reward = -1.0\n",
        "          done = True\n",
        "\n",
        "          # this section is for display purposes\n",
        "          # clear_screen()\n",
        "          self.cumulative_reward += reward\n",
        "          print(f'Cumulative Reward: {self.cumulative_reward}, Result: LOSE')\n",
        "      #\n",
        "      # Update the environment state\n",
        "      #\n",
        "      if not done:\n",
        "          reward = -0.1\n",
        "          # update the player position\n",
        "          self.state[previous_position] = NOTHING\n",
        "          self.state[self.player_position] = PLAYER\n",
        "          self.cumulative_reward += reward\n",
        "\n",
        "      return self.state, reward, done, False, info\n",
        "\n",
        "    def reset(self):\n",
        "      self.cumulative_reward = 0\n",
        "      #\n",
        "      # set the initial state to a flattened 6x6 grid with a randomly\n",
        "      # placed entry, win, and player\n",
        "      #\n",
        "      self.state = [NOTHING] * 36\n",
        "      # self.player_position = random.randrange(0, 36)\n",
        "      # self.win_position = random.randrange(0, 36)\n",
        "      # self.lose_position = random.randrange(0, 36)\n",
        "\n",
        "      self.player_position = 5\n",
        "      self.win_position = 15\n",
        "      self.lose_position = 30\n",
        "\n",
        "      # make sure the entry and lose points aren't\n",
        "      # overlapping each other\n",
        "      while self.win_position == self.player_position:\n",
        "          self.win_position = random.randrange(0, 36)\n",
        "      while self.lose_position == self.win_position or self.lose_position == self.player_position:\n",
        "          self.lose_position = random.randrange(0, 36)\n",
        "\n",
        "      self.state[self.player_position] = PLAYER\n",
        "      self.state[self.win_position] = WIN\n",
        "      self.state[self.lose_position] = LOSE\n",
        "\n",
        "      # convert the python array into a numpy array\n",
        "      # (needed since Gym expects the state to be this way)\n",
        "      self.state = np.array(self.state, dtype=np.int16)\n",
        "      return self.state\n",
        "\n",
        "    def render(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaISx6Gmx3YQ"
      },
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "env = GameBoardEnv()\n",
        "\n",
        "obs = env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5XoXAdPl9XH"
      },
      "outputs": [],
      "source": [
        "input_shape = env.observation_space.shape\n",
        "n_outputs = env.action_space.n\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(36, activation=\"elu\", input_shape=input_shape),\n",
        "    tf.keras.layers.Dense(32, activation=\"elu\"),\n",
        "    tf.keras.layers.Dense(n_outputs)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJ9DOO-imFRp"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy_policy(state, epsilon=0):\n",
        "    # if np.random.rand() < epsilon:\n",
        "        return np.random.randint(4)  # random action\n",
        "    # else:\n",
        "    #     Q_values = model.predict(state[np.newaxis], verbose=0)[0]\n",
        "    #     return Q_values.argmax()  # optimal action according to the DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMZZnrqMmMBx"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "\n",
        "replay_buffer = deque(maxlen=2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haAZtZrUmQZW"
      },
      "outputs": [],
      "source": [
        "def sample_experiences(batch_size):\n",
        "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
        "    batch = [replay_buffer[index] for index in indices]\n",
        "    return [\n",
        "        np.array([experience[field_index] for experience in batch])\n",
        "        for field_index in range(6)\n",
        "    ]  # [states, actions, rewards, next_states, dones, truncateds]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6w5ypOymSu2"
      },
      "outputs": [],
      "source": [
        "def play_one_step(env, state, epsilon):\n",
        "    action = epsilon_greedy_policy(state, epsilon)\n",
        "    next_state, reward, done, truncated, info = env.step(action)\n",
        "    # if done and env.cumulative_reward > 0:\n",
        "    replay_buffer.append((state, action, reward, next_state, done, truncated))\n",
        "    return next_state, reward, done, truncated, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJa1Tu7FmZcW"
      },
      "outputs": [],
      "source": [
        "batch_size = 35\n",
        "discount_factor = 0.95\n",
        "optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-2)\n",
        "loss_fn = tf.keras.losses.mean_squared_error\n",
        "# loss_fn = tf.keras.losses.hinge\n",
        "# loss_fn = tf.keras.losses.categorical_crossentropy\n",
        "\n",
        "def training_step(batch_size):\n",
        "    experiences = sample_experiences(batch_size)\n",
        "    states, actions, rewards, next_states, dones, truncateds = experiences\n",
        "    next_Q_values = model.predict(next_states, verbose=0)\n",
        "    max_next_Q_values = next_Q_values.max(axis=1)\n",
        "    runs = 1.0 - (dones | truncateds)  # episode is not done or truncated\n",
        "    target_Q_values = rewards + runs * discount_factor * max_next_Q_values\n",
        "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
        "    mask = tf.one_hot(actions, n_outputs)\n",
        "    with tf.GradientTape() as tape:\n",
        "        all_Q_values = model(states)\n",
        "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
        "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaM5Xfc3oJDt"
      },
      "outputs": [],
      "source": [
        "obs = env.reset()\n",
        "# obs, info = env.reset(seed=42)\n",
        "# env.reset()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "rewards = []\n",
        "best_score = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tT0adv7Pmc_q",
        "outputId": "b0af1483-f826-439e-cfb4-77802964f741"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cumulative Reward: -2.900000000000002, Result: WIN\n",
            "\rEpisode: 0, Steps: 40, eps: 1.000\n",
            "\n",
            "Cumulative Reward: -0.30000000000000004, Result: WIN\n",
            "\rEpisode: 1, Steps: 14, eps: 0.998\n",
            "\n",
            "Cumulative Reward: -2.0000000000000013, Result: WIN\n",
            "\rEpisode: 2, Steps: 31, eps: 0.996\n",
            "\n",
            "Cumulative Reward: -1.600000000000001, Result: WIN\n",
            "\rEpisode: 3, Steps: 27, eps: 0.994\n",
            "\n",
            "Cumulative Reward: -1.1000000000000005, Result: WIN\n",
            "\rEpisode: 4, Steps: 22, eps: 0.992\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for episode in range(5):\n",
        "  obs = env.reset()\n",
        "  for step in range(100):\n",
        "    epsilon = max(1 - episode / 500, 0.01)\n",
        "    obs, reward, done, truncated, info = play_one_step(env, obs, epsilon)\n",
        "    if done or truncated:\n",
        "      break\n",
        "\n",
        "  print(\"\\rEpisode: {}, Steps: {}, eps: {:.3f}\".format(episode, step + 1, epsilon), end=\"\") # Not shown\n",
        "  print(\"\\n\") # Not shown\n",
        "\n",
        "  rewards.append(env.cumulative_reward) # Not shown in the book\n",
        "  if env.cumulative_reward >= best_score: # Not shown\n",
        "      best_weights = model.get_weights() # Not shown\n",
        "      best_score = env.cumulative_reward # Not shown\n",
        "\n",
        "  if episode > 50:\n",
        "      training_step(batch_size)\n",
        "\n",
        "# model.set_weights(best_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBr3ewDQrBSo"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(rewards)\n",
        "plt.xlabel(\"Episode\", fontsize=14)\n",
        "plt.ylabel(\"Sum of rewards\", fontsize=14)\n",
        "# save_fig(\"dqn_rewards_plot\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNx3yYsq4j7iDfwv+rCnIX+"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}